{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
      "2110848/2110848 [==============================] - 2s 1us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import reuters\n",
    "from keras.utils import pad_sequences\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "#Data Loading from keras datasets\n",
    "max_features = 10000\n",
    "maxlen = 100\n",
    "num_classes = 46\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_features, test_split=0.2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing is an essential step in any machine learning project. \n",
    "It includes:\n",
    "\n",
    "1)Data cleaning: This involves identifying and handling missing or erroneous data, such as removing duplicates, imputing missing values, and dealing with outliers.\n",
    "\n",
    "2)Feature Engineering Which Includes\n",
    "    a)Feature scaling: Different features may have different scales, which can affect the performance of some machine learning algorithms. Common methods of feature scaling include normalization and standardization.\n",
    "\n",
    "    b)Feature encoding: Machine learning algorithms typically require numerical data, so categorical variables need to be encoded. This can be done using methods like one-hot encoding, label encoding, and target encoding.\n",
    "\n",
    "3)Train-test split: It's important to split your data into separate training and testing sets to evaluate the performance of your model. Typically, around 80% of the data is used for training and 20% for testing.\n",
    "\n",
    "4)Data Augumentation: Data augmentation is a technique used to increase the size of a training dataset by generating new examples through transformations of the existing data. This can help to prevent overfitting and improve the performance of machine learning models.\n",
    "\n",
    "By paying attention to these main points, you can ensure that your data is properly prepared for machine learning and that your models are as accurate and reliable as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 32)           320000    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3200)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                204864    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 46)                2990      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 527,854\n",
      "Trainable params: 527,854\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model = Sequential([\n",
    "    Embedding(max_features, 32, input_length=maxlen),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "281/281 [==============================] - 5s 17ms/step - loss: 0.2415 - accuracy: 0.9493 - val_loss: 1.4016 - val_accuracy: 0.6910\n",
      "Epoch 2/5\n",
      "281/281 [==============================] - 4s 14ms/step - loss: 0.2009 - accuracy: 0.9519 - val_loss: 1.3571 - val_accuracy: 0.7115\n",
      "Epoch 3/5\n",
      "281/281 [==============================] - 4s 14ms/step - loss: 0.1867 - accuracy: 0.9509 - val_loss: 1.4318 - val_accuracy: 0.6986\n",
      "Epoch 4/5\n",
      "281/281 [==============================] - 4s 14ms/step - loss: 0.1708 - accuracy: 0.9515 - val_loss: 1.4360 - val_accuracy: 0.6968\n",
      "Epoch 5/5\n",
      "281/281 [==============================] - 4s 14ms/step - loss: 0.1641 - accuracy: 0.9520 - val_loss: 1.3812 - val_accuracy: 0.7119\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_test, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we loaded the Reuters newswire dataset from keras. Then we preprocessed the data. Then, we \n",
    "built a model using keras. After that we trained the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
